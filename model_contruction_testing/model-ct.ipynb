{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers -q\n!pip install keras-tcn -q\nfrom tcn import TCN\nimport pandas as pd \nimport numpy as np \nimport tensorflow as tf \nimport re \nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert class in text to numbers\ndef get_fnn_class(row):\n    if (row['label_fnn'] == 'fake'):\n        return 0\n    else:\n        return 1\n    \ndef get_liar_poli_class(row, label):\n    # false, pants-fire, barely-true,\n    # true, half-true, mostly-true\n    if (row[label] == 'false' or row[label] == 'pants-fire' or row[label] == 'barely-true'):\n        return 0\n    else:\n        return 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# load data\ncnn_news = pd.read_csv('../input/breaking-news-from-twitter-20102021/tweets_cnn.csv')\n\nsnopes = pd.read_csv('../input/scraped-fake/snopes-scrape-full.csv')\naff = pd.read_csv('../input/scraped-fake/aff-scrape-full-processed.csv')\n\ncovid_ifcn = pd.read_csv('../input/scraped-fake/ifcn-scrape-full.csv')\ncovid_real = pd.read_csv('../input/covidnews/trueNews.csv')\n\nfn_get_real = pd.read_csv('../input/fake-news/fake.csv')\n\nfnnn_train = pd.read_csv('../input/fnnndata/fnn_train.csv', index_col=0)\nfnnn_test = pd.read_csv('../input/fnnndata/fnn_test.csv', index_col=0)\nfnnn_dev = pd.read_csv('../input/fnnndata/fnn_dev.csv', index_col=0)\n\nliar_data_train = pd.read_csv('../input/liardata/liar_train.csv', index_col=0)\nliar_data_test = pd.read_csv('../input/liardata/liar_test.csv', index_col=0)\nliar_data_dev = pd.read_csv('../input/liardata/liar_dev.csv', index_col=0)\n\npolitifact = pd.read_csv('../input/politifact/politifact-scrape-full.csv')\npoli_alt = pd.read_csv('../input/politifact-factcheck-data/politifact.csv')\n\nfnn_buzzfake = pd.read_csv('../input/fakenewsnet/BuzzFeed_fake_news_content.csv')\nfnn_buzzreal = pd.read_csv('../input/fakenewsnet/BuzzFeed_real_news_content.csv')\n\nfr_fake = pd.read_csv('../input/fake-and-real-news-dataset/Fake.csv')\nfr_real = pd.read_csv('../input/fake-and-real-news-dataset/True.csv')\nbbc_news = pd.read_csv('../input/breaking-news-from-twitter-20102021/tweets_bbc.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# prepare data\npoli_liar_fnn = None\nscraped_fake = None\n\n# buzzfeed data\nfnn_buzzfake = fnn_buzzfake[['title']]\nfnn_buzzreal = fnn_buzzreal[['title']]\nfnn_buzzfake['class'] = 0 \nfnn_buzzreal['class'] = 1\npoli_liar_fnn = pd.concat([fnn_buzzfake, fnn_buzzreal], ignore_index=True, sort=False)\n\n# get data needed from FNN dataset\nfnnn_train['class'] = fnnn_train.apply(lambda row: get_fnn_class(row), axis=1)\nfnnn_test['class'] = fnnn_test.apply(lambda row: get_fnn_class(row), axis=1)\nfnnn_dev['class'] = fnnn_dev.apply(lambda row: get_fnn_class(row), axis=1)\nfnnn_train = fnnn_train.rename({'statement' : 'title'}, axis=1)\nfnnn_test = fnnn_test.rename({'statement' : 'title'}, axis=1)\nfnnn_dev = fnnn_dev.rename({'statement' : 'title'}, axis=1)\n\nfnnn_train = fnnn_train[['title', 'class']]\nfnnn_test = fnnn_test[['title', 'class']]\nfnnn_dev = fnnn_dev[['title', 'class']]\ncombine_fnnn_list = [fnnn_train, fnnn_test, fnnn_dev]\ncombined_fnnn_data = pd.concat(combine_fnnn_list, ignore_index=True, sort=False)\npoli_liar_fnn = pd.concat([poli_liar_fnn, combined_fnnn_data], ignore_index=True, sort=False)\n\n# get the rest of politifact data\npolitifact = politifact.drop(politifact[(politifact.target == 'full-flop') | (politifact.target == 'half-flip') | (politifact.target == 'no-flip')].index)\npolitifact['class'] = politifact.apply(lambda row: get_liar_poli_class(row, 'target'), axis=1)\npolitifact = politifact.rename({'statement' : 'title'}, axis=1)\npolitifact = politifact[['title', 'class']]\npoli_liar_fnn = pd.concat([poli_liar_fnn, politifact], ignore_index=True, sort=False)\n\n# get more politifact data\npoli_alt = poli_alt.drop(poli_alt[(poli_alt.fact == 'full-flop') | (poli_alt.fact == 'half-flip') | (poli_alt.fact == 'no-flip')].index)\npoli_alt['class'] = poli_alt.apply(lambda row: get_liar_poli_class(row, 'fact'), axis=1)\npoli_alt = poli_alt.rename({'sources_quote' : 'title'}, axis=1)\npoli_alt = poli_alt[['title', 'class']]\npoli_liar_fnn = pd.concat([poli_liar_fnn, poli_alt], ignore_index=True, sort=False)\n\n# get data needed from LIAR dataset\nliar_data_train['class'] = liar_data_train.apply(lambda row: get_liar_poli_class(row, 'label-liar'), axis=1)\nliar_data_test['class'] = liar_data_test.apply(lambda row: get_liar_poli_class(row, 'label-liar'), axis=1)\nliar_data_dev['class'] = liar_data_dev.apply(lambda row: get_liar_poli_class(row, 'label-liar'), axis=1)\nliar_data_train = liar_data_train.rename({'statement' : 'title'}, axis=1)\nliar_data_test = liar_data_test.rename({'statement' : 'title'}, axis=1)\nliar_data_dev = liar_data_dev.rename({'statement' : 'title'}, axis=1)\nliar_data_train = liar_data_train[['title', 'class']]\nliar_data_test = liar_data_test[['title', 'class']]\nliar_data_dev = liar_data_dev[['title', 'class']]\ncombine_liar_list = [liar_data_train, liar_data_test, liar_data_dev]\ncombined_liar_data = pd.concat(combine_liar_list, ignore_index=True, sort=False)\npoli_liar_fnn = pd.concat([poli_liar_fnn, combined_liar_data], ignore_index=True, sort=False)\npoli_liar_fnn = poli_liar_fnn.drop_duplicates(subset='title', ignore_index=True)\npoli_liar_fnn = poli_liar_fnn[poli_liar_fnn.title.str.count(' ').gt(5)]\n\n# get data from getting real dataset\nfn_get_real = fn_get_real[['title']]\nfn_get_real['class'] = 0 \nscraped_fake = pd.concat([scraped_fake, fn_get_real], ignore_index=True, sort=False)\n\n# add all the scraped fake news\nscraped_fake = pd.concat([scraped_fake, aff], ignore_index=True, sort=False)\nscraped_fake = pd.concat([scraped_fake, snopes], ignore_index=True, sort=False)\nscraped_fake = scraped_fake.drop_duplicates(subset='title', ignore_index=True)\nscraped_fake = scraped_fake[scraped_fake.title.str.count(' ').gt(5)]\n\n# add real news from cnn\ncnn_news = cnn_news.rename({'tweet' : 'title'}, axis=1)\ncnn_news = cnn_news[['title']]\ncnn_news['class'] = 1\ncnn_news = cnn_news.drop_duplicates(subset='title', ignore_index=True)\ncnn_news = cnn_news[cnn_news.title.str.count(' ').gt(5)]\n\n# split covid data into train and test set\ncovid_real = covid_real.rename({'Text' : 'title'}, axis=1)\ncovid_real = covid_real.rename({'Label' : 'class'}, axis=1)\ncovid_real = covid_real[['title', 'class']]\ncombined_covid_data = pd.concat([covid_ifcn, covid_real], ignore_index=True, sort=False)\ncombined_covid_data = combined_covid_data.drop_duplicates(subset='title', ignore_index=True)\ncombined_covid_data = combined_covid_data[combined_covid_data.title.str.count(' ').gt(5)]\n\n# get data needed from fr dataset\nfr_fake = fr_fake[['title']]\nfr_real = fr_real[['title']]\nfr_fake['class'] = 0 \nfr_real['class'] = 1\nfr_data = pd.concat([fr_fake, fr_real], ignore_index=True, sort=False)\nfr_data = fr_data.drop_duplicates(subset='title', ignore_index=True)\n\n# get data from BBC\nbbc_news = bbc_news.rename({'tweet' : 'title'}, axis=1)\nbbc_news = bbc_news[['title']]\nbbc_news['class'] = 1 \nbbc_news = bbc_news.drop_duplicates(subset='title', ignore_index=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_covid_data = combined_covid_data['title'].astype(str)\ntargets_covid_data = combined_covid_data['class']\ncovid_feature_train, covid_feature_test, covid_target_train, covid_target_test = train_test_split(features_covid_data, targets_covid_data, test_size=0.10, stratify=targets_covid_data, random_state=42)\n\nfeatures_poli_liar_fnn = poli_liar_fnn['title'].astype(str)\ntargets_poli_liar_fnn = poli_liar_fnn['class']\npln_feature_train, pln_feature_test, pln_target_train, pln_target_test = train_test_split(features_poli_liar_fnn, targets_poli_liar_fnn, test_size=0.10, stratify=targets_poli_liar_fnn, random_state=42)\n\nfeatures_scraped_fake = scraped_fake['title'].astype(str)\ntargets_scraped_fake = scraped_fake['class']\nfake_feature_train, fake_feature_test, fake_target_train, fake_target_test = train_test_split(features_scraped_fake, targets_scraped_fake, test_size=0.10, stratify=targets_scraped_fake, random_state=42)\n\nfeature_cnn = cnn_news['title'].astype(str)\ntarget_cnn = cnn_news['class']\n\nfeatures_fr_data = fr_data['title'].astype(str)\ntargets_fr_data = fr_data['class']\n\nfeature_bbc = bbc_news['title'].astype(str)\ntarget_bbc = bbc_news['class']\n\nfeature_train = pd.concat([covid_feature_train, pln_feature_train, fake_feature_train, feature_cnn], ignore_index=True, sort=False)\ntarget_train = pd.concat([covid_target_train, pln_target_train, fake_target_train, target_cnn], ignore_index=True, sort=False)\n\nfeature_val = pd.concat([covid_feature_test, pln_feature_test, fake_feature_test], ignore_index=True, sort=False)\ntarget_val = pd.concat([covid_target_test, pln_target_test, fake_target_test], ignore_index=True, sort=False)\n\nfeature_test = pd.concat([covid_feature_test, pln_feature_test, fake_feature_test, features_fr_data, feature_bbc], ignore_index=True, sort=False)\ntarget_test = pd.concat([covid_target_test, pln_target_test, fake_target_test, targets_fr_data, target_bbc], ignore_index=True, sort=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import wordnet\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem import WordNetLemmatizer\n\nstop_words = set(stopwords.words('english'))\nstop_words.remove(\"not\")\nkeep_list = [\"aren't\", \"don't\", \"didn't\", \"hasn't\", \"wouldn't\", \"weren't\", \"not\", \"isn't\", \"couldn't\", \n             \"shouldn't\", \"won't\", \"mustn't\", \"wasn't\", \"haven't\", \"doesn't\", \"hadn't\", \"needn't\",\n             \"didn\", \"wasn\", \"isn\", \"hasn\", \"needn\", \"shouldn\", \"couldn\", \"wouldn\", \"weren\", \"haven\", \"aren\", \"doesn\", \"mustn\", \"mightn\"]\ntweet_tokenizer = TweetTokenizer(reduce_len=True)\nwordnet_lemmatizer = WordNetLemmatizer()\ntag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n\ndef get_pos_tag(word):\n    return tag_dict.get(nltk.pos_tag([word])[0][1][0].upper(), wordnet.NOUN)\n\ndef seperate_texts(text):\n    return tweet_tokenizer.tokenize(text)\n\ndef lemmatize_nots(text):\n    for word in keep_list:\n        text = text.replace(word, \"not\")\n    return text\n\ndef lemmatize(word_list):\n    return [wordnet_lemmatizer.lemmatize(word, get_pos_tag(word)) for word in word_list]\n\ndef remove_stopwords_and_others(word_list):\n    return [word for word in word_list if word not in stop_words and \"http\" not in word and \"@\" not in word]\n\ndef process(text):\n    text = lemmatize_nots(text)\n    word_list = seperate_texts(text)\n    result = remove_stopwords_and_others(word_list)\n    lemmatized_result = lemmatize(result)\n    data = \" \". join(lemmatized_result)\n    return data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BertTokenizer, AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\", use_fast=False)\ntokenizer.padding_side='right'\n\ndef pre_process(data):\n    result = []\n    for text in data:\n        text = text.lower()\n        text = re.sub(\"'\", ' ', text)\n        text = process(text)\n        text = re.sub(\"\\\\W\", ' ', text)\n        # get rid of extra spaces\n        text = re.sub(' +', ' ', text)\n        text = re.sub('^ ', '', text)\n        text = re.sub(' $', '', text)\n        result.append(text)\n    return result\n\ndef tokenise(features):\n    f = []\n    features = pre_process(features)\n    for feature in features:\n        f.append(tokenizer.encode(feature, max_length=64, pad_to_max_length=True, truncation=True))\n    return f","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_train = np.array(tokenise(feature_train))\nfeature_val = np.array(tokenise(feature_val))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0, \n                                             patience=10, verbose=2, \n                                             mode='auto', restore_best_weights=True)\n# build model\ndef build_tcn():\n    inputs = tf.keras.layers.Input(shape=(64,))\n    embed = tf.keras.layers.Embedding(100000, 300)(inputs)\n    sd = tf.keras.layers.SpatialDropout1D(0.2)(embed)\n    tcn = TCN(nb_filters=128, dilations = [1, 2, 4], return_sequences=True, use_batch_norm=True)(sd)\n    ap = tf.keras.layers.GlobalAveragePooling1D()(tcn)\n    mp = tf.keras.layers.GlobalMaxPooling1D()(tcn)    \n    conc = tf.keras.layers.concatenate([ap, mp])\n    dense = tf.keras.layers.Dense(16, activation=\"relu\")(conc)\n    dropout = tf.keras.layers.Dropout(0.2)(dense)\n    outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dropout)    \n    tcn_model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n    return tcn_model\n\n# model training\ntcn_model = build_tcn()\ntcn_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n                  optimizer=tf.keras.optimizers.Adam(1e-4), \n                  metrics=['accuracy'])\ntcn_model.fit(feature_train, target_train, epochs=50, validation_data=(feature_val, target_val), \n              batch_size=32, callbacks=[callbacks], shuffle=True)\ntcn_model.save('tcn.h5')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # bidirectional LSTM\n# biLSTM = tf.keras.Sequential([\n#     tf.keras.layers.Input(shape=(64,)),\n#     tf.keras.layers.Embedding(50000, 300),\n#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n#     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16)),\n#     tf.keras.layers.Dense(16, activation='relu'),\n#     tf.keras.layers.Dropout(0.2),\n#     tf.keras.layers.Dense(1, activation=\"sigmoid\")\n# ])\n# biLSTM.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n#               optimizer=tf.keras.optimizers.Adam(1e-4),\n#               metrics=['accuracy'])\n\n# bl = biLSTM.fit(feature_train, target_train, epochs=50, validation_data=(feature_val, target_val), batch_size=32, callbacks=[callbacks], shuffle=True)\n\n# # LSTM\n# LSTM = tf.keras.Sequential([\n#     tf.keras.layers.Input(shape=(64,)),\n#     tf.keras.layers.Embedding(50000, 300),\n#     tf.keras.layers.LSTM(64, return_sequences=True),\n#     tf.keras.layers.LSTM(16),\n#     tf.keras.layers.Dense(16, activation='relu'),\n#     tf.keras.layers.Dropout(0.2),\n#     tf.keras.layers.Dense(1, activation=\"sigmoid\")\n# ])\n# LSTM.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n#               optimizer=tf.keras.optimizers.Adam(1e-4),\n#               metrics=['accuracy'])\n\n# LSTM.fit(feature_train, target_train, epochs=50, validation_data=(feature_val, target_val), batch_size=32, callbacks=[callbacks], shuffle=True)\n\n# # GRU\n# GRU = tf.keras.Sequential([\n#     tf.keras.layers.Input(shape=(64,)),\n#     tf.keras.layers.Embedding(50000, 300),\n#     tf.keras.layers.GRU(64, return_sequences=True),\n#     tf.keras.layers.GRU(16),\n#     tf.keras.layers.Dense(16, activation='relu'),\n#     tf.keras.layers.Dropout(0.2),\n#     tf.keras.layers.Dense(1, activation=\"sigmoid\")\n# ])\n# GRU.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n#               optimizer=tf.keras.optimizers.Adam(1e-4),\n#               metrics=['accuracy'])\n\n# GRU.fit(feature_train, target_train, epochs=50, validation_data=(feature_val, target_val), batch_size=32, callbacks=[callbacks], shuffle=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_fr_data = np.array(tokenise(features_fr_data))\ncovid_feature_test = np.array(tokenise(covid_feature_test))\npln_feature_test = np.array(tokenise(pln_feature_test))\nfake_feature_test = np.array(tokenise(fake_feature_test))\nfeature_bbc = np.array(tokenise(feature_bbc))\nfeature_test = np.array(tokenise(feature_test))\n\ntcn_model.evaluate(features_fr_data, targets_fr_data)\ntcn_model.evaluate(covid_feature_test, covid_target_test)\ntcn_model.evaluate(pln_feature_test, pln_target_test)\ntcn_model.evaluate(fake_feature_test, fake_target_test)\ntcn_model.evaluate(feature_bbc, target_bbc)\ntcn_model.evaluate(feature_test, target_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def incremental_train(model, features, targets, epochs, lr):\n    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n                  optimizer=tf.keras.optimizers.Adam(lr), \n                  metrics=['accuracy'])\n    model.fit(features, targets, epochs=epochs, batch_size=32, shuffle=True)\n    return model\n\n\ndef analyse_pheme_data(event, model, propotion_to_train, epochs, lr):\n    fake = pd.read_csv(\"../input/pheme-data/\" + event + \"-rumours.csv\")\n    real = pd.read_csv(\"../input/pheme-data/\" + event + \"-non-rumours.csv\")\n    \n    fake_number = int(fake.shape[0]*propotion_to_train)\n    real_number = int(real.shape[0]*propotion_to_train)\n\n    fake = fake.rename({'text' : 'title'}, axis=1)\n    fake = fake[['title', 'class']]\n    real = real.rename({'text' : 'title'}, axis=1)\n    real = real[['title', 'class']]\n    data_train = pd.concat([fake[:fake_number], real[:real_number]], ignore_index=True, sort=False)\n    data_test = pd.concat([fake[fake_number:], real[real_number:]], ignore_index=True, sort=False)\n\n    features_train = data_train['title'].astype(str)\n    targets_train = data_train['class']\n    features_train = np.array(tokenise(features_train))\n    \n    features_test = data_test['title'].astype(str)\n    targets_test = data_test['class']\n    features_test = np.array(tokenise(features_test))\n    \n    print(features_train.shape)\n    print(features_test.shape)\n    \n    print(\"without training\")\n    model.evaluate(features_test, targets_test)\n    trained_model = incremental_train(model, features_train, targets_train, epochs, lr)\n    print(\"with training\")\n    loss, accuracy = trained_model.evaluate(features_test, targets_test)\n    t_loss, t_accuracy = trained_model.evaluate(feature_test, target_test)\n    return accuracy, t_accuracy","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"events = [\"germanwings-crash\", \"ferguson\", \"charliehebdo\", \"putinmissing\", \"ottawashooting\", \"sydneysiege\"]\nepochs_list = [5, 10, 15, 20, 25, 30]\nlr_list = [1e-3, 1e-4, 5e-5, 1e-5, 5e-6, 1e-6]\n\navg_accs = []\naccs_for_combined = []\nfor epochs in epochs_list:\n    for lr in lr_list:\n        sum_acc = 0\n        t_sum_acc = 0\n        for event in events:\n            model = tf.keras.models.load_model('../input/models-tcn/tcn_model.h5', custom_objects={'TCN': TCN})\n            print(\"analysing: \" + event)\n            accuracy, t_accuracy = analyse_pheme_data(event, model, 0.2, epochs, lr)\n            sum_acc += accuracy\n            t_sum_acc += t_accuracy\n        avg_accs.append((epochs, lr, sum_acc/6, t_sum_acc/6))\n\nfor acc in avg_accs:\n    print(acc)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def analyse_pheme_data_inc(event, model, trained_model, iteration, epochs, lr):\n    fake = pd.read_csv(\"../input/pheme-data/\" + event + \"-rumours.csv\")\n    real = pd.read_csv(\"../input/pheme-data/\" + event + \"-non-rumours.csv\")\n\n    fake = fake.rename({'text' : 'title'}, axis=1)\n    fake = fake[['title', 'class']]\n    real = real.rename({'text' : 'title'}, axis=1)\n    real = real[['title', 'class']]\n    data = pd.concat([fake, real], ignore_index=True, sort=False)\n    features = data['title'].astype(str)\n    targets = data['class']\n    features = np.array(tokenise(features))\n    \n   \n    size_list = []\n    acc_list = []\n    acc_list_no_train = []\n    data_per_iter = int(data.shape[0]/iteration)\n    \n    for i in range(iteration):\n        print(\"iteration: \" + str(i))\n        if i == iteration-1:\n            X_train, X_test, y_train, y_test = train_test_split(features, targets, train_size=0.2, random_state=42, stratify=targets)\n            size_list.append((features.shape[0], targets.shape[0], X_train.shape[0], X_test.shape[0]))\n        else:\n            cur_size = data_per_iter/features.shape[0]\n            X_cur, X_left, y_cur, y_left = train_test_split(features, targets, train_size=cur_size, random_state=42, stratify=targets)\n            X_train, X_test, y_train, y_test = train_test_split(X_cur, y_cur, train_size=0.2, random_state=42, stratify=y_cur)\n            size_list.append((X_cur.shape[0], X_left.shape[0], X_train.shape[0], X_test.shape[0]))\n            \n        print(\"without training\")\n        loss_no_train, accuracy_no_train = model.evaluate(X_test, y_test)\n        acc_list_no_train.append(accuracy_no_train)\n        \n        trained_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n                  optimizer=tf.keras.optimizers.Adam(lr), \n                  metrics=['accuracy'])\n        trained_model.fit(X_train, y_train, epochs=epochs, batch_size=32, shuffle=True)\n        print(\"with training\")\n        loss, accuracy = trained_model.evaluate(X_test, y_test) \n        acc_list.append(accuracy)\n        features = X_left\n        targets = y_left\n    \n    return (size_list, acc_list_no_train, acc_list)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"events = [(\"germanwings-crash\", 10), (\"ferguson\", 19), (\"charliehebdo\", 33), (\"putinmissing\", 4), (\"ottawashooting\", 20), (\"sydneysiege\", 25)]\nno_train_avg = []\ntrain_avg = []\naccuracy_data = []\n\nfor event in events:\n    model = tf.keras.models.load_model('../input/models-tcn/tcn_model.h5', custom_objects={'TCN': TCN})\n    trained_model = tf.keras.models.load_model('../input/models-tcn/tcn_model.h5', custom_objects={'TCN': TCN})\n    print(\"analysing: \" + event[0])\n    accuracy_list = analyse_pheme_data_inc(event[0], model, trained_model, event[1], 15, 0.0001)\n    accuracy_data.append(accuracy_list)\n    no_train_avg.append(sum(accuracy_list[1])/len(accuracy_list[1]))\n    train_avg.append(sum(accuracy_list[2])/len(accuracy_list[2]))\n    \nfor i in range(6):\n    print(events[i])\n    print(no_train_avg[i])\n    print(train_avg[i])\n    print(accuracy_data[i][0])\n    print(accuracy_data[i][1])\n    print(accuracy_data[i][2])","metadata":{},"execution_count":null,"outputs":[]}]}